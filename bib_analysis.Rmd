---
title: "Analysis of climate justice literature"
output: html_document
---

<!-- # Workflow (notes to self) -->

<!-- - Number of articles through time -->

<!-- - Which journals are being published in bar graph -->

<!-- - mean number of authors per articles -->

<!-- - Author collaboration network (tidygraph/tidygraph) -->

<!-- - Institution not always linked to country. George has code -->

<!-- - George has code to clean language special characters -->

<!-- - Finn did author initials cleaning code -->

<!-- - Keyword co-occurance (can split out by time) -->

<!-- - ngrams (whole abstract; tidytex) -->

<!-- - LDA topic within topic -->

<!-- - DO THIS BETORE TOPIC MODELLING Stopwords (throw out the, a, and ...) and lemitize (contractions), another that does run/ran (tidytex) -->

<!-- - revtools package -->


# Info

I'm doing an analysis of the literature published on 'climate justice', including 'social justice', 'environmental justice', 'climate change', and 'global warming'. The search terms and query use in the Scopus API were provided by Meg Parsons and returns 1827 results. Meg is interested in most of the standard info like when 'climate justice' was first mentioned, how popular the topic has become, who is publishing on it and so on. I'm hoping to delve a little deeper with some text mining analysis. This .rmd is the latest attempt after trying the bibliometrix package approach.

I'd appriciate:

- An eye out for errors (I'm not sure if all the functions do what I expect them too, e.g., lemmatisation, incorrect grouping by article)
  - I think I'm missing a filtering step to remove some poor records like auther name = The Lancet

- Any leads/code/resources on sections I haven't completed yet marked by empty headers at the end (like plotting the author network)

- Methods of analysing the topic (I've seen ordinations on topic analysis)

- Any useful stuff I haven't thought of doing!

I ditched my first attempt following the bibliometrix package approach and the following draws heavily on [this tutorial](https://bookdown.org/Maxine/tidy-text-mining/tidy-text-format.html). Manually downloaded records from Scopus were not complete or tidy so I took George's advice and turned to using the API and focusing on Scopus and not Web of Science (at least for now). 


```{r include=FALSE}
library(tidyverse)
# library(data.table)
library(janitor) # Cleaning names
library(rscopus) # For querying scopus through the API 
library(lubridate) # Date formats
library(tidytext) # Text mining
library(ggwordcloud) # plotting word clouds
# library(ggthemes)
library(tidylo)
library(widyr) # Word matrix
library(tidygraph)
library(ggraph)
library(patchwork)

# plotting function from tutorial
facet_bar <- function(df, y, x, by, nrow = 2, ncol = 2, scales = "free") {
  mapping <- aes(y = reorder_within({{ y }}, {{ x }}, {{ by }}), 
                 x = {{ x }}, 
                 fill = {{ by }})
  
  facet <- facet_wrap(vars({{ by }}), 
                      nrow = nrow, 
                      ncol = ncol,
                      scales = scales) 
  
  ggplot(df, mapping = mapping) + 
    geom_col(show.legend = FALSE) + 
    scale_y_reordered() + 
    facet + 
    ylab("")
}
```


# Corpus overview

```{r include=FALSE}
# The following code requires on-campus network access. Work around for lockdown was to use a nectar VM. Using the UoA squid proxy should work as well. The data from the query have been downloaded and saved so the following code is commented out. 1827 results have been returned


# rscopus::set_api_key("cf71c76658efeaf1c2da32d64bc8e1d6")
# scopus_query <- scopus_search(query = "TITLE-ABS-KEY ( \"climate justice\"  OR  
#                        \"environmental justice\"  OR  \"social justice\"  AND  
#                        \"climate change\"  OR  \"Global warming\" )  AND  
#                        ( LIMIT-TO ( LANGUAGE ,  \"English\" ) )", 
#                        view = "COMPLETE", count = 25, max_count = 2500)
# 
# scopus_results <- gen_entries_to_df(res70$entries)
```


```{r include=FALSE}
# Some formatting/wrangling

scopus_results <- readRDS("data/scopus_api_1827.rds")
scopus_results <- lapply(scopus_results, clean_names)
str(scopus_results[[1]])
str(scopus_results$affiliation)
str(scopus_results$author)
str(scopus_results$`prism:isbn`)
lapply(scopus_results, str)

# The following does not clean all columns to the correct format but is a start.
author_df <- scopus_results[[1]] |> 
  mutate(prism_cover_date = as_date(prism_cover_date),
         author_count_total = as.numeric(author_count_total),
         year = floor_date(prism_cover_date, "year"),
         prism_issn = as.numeric(prism_e_issn),
         prism_e_issn = as.numeric(prism_e_issn),
         prism_volume = as.numeric(prism_volume),
         citedby_count = as.numeric(citedby_count)
         )


author_df[which(is.na(author_df$subtype_description)),]

table(author_df$subtype, exclude = NULL) # only 1 undefined
table(author_df$subtype_description, exclude = NULL) # undefined shows as NA

# Checking out which fields have lots of NA values
sapply(author_df, function(x) sum(is.na(x)))

```


## What publication types (books, artilces...)

Filter out Notes, Letter, Erratum, Short Survey, Conference Paper, Conference Review?

```{r echo=FALSE, warning=FALSE}
ggplot(author_df, aes(fct_infreq(subtype_description))) +
  geom_bar() +
  scale_x_discrete(na.translate = FALSE) + # Leave out the 1 NA
  theme_minimal()
```


## Top 20 most popular journals

```{r echo=FALSE}
# ggplot(author_df, aes(fct_infreq(prism_publication_name))) +
#   geom_bar() +
#   scale_x_discrete()

journal_df <- author_df |>
  count(prism_publication_name, sort = TRUE) |>
  mutate(prism_publication_name = fct_reorder(prism_publication_name, n))

# ggplot(tail(journal_df, 20), aes(x = prism_publication_name, y = n)) +
#   geom_bar(stat = "identity") +
#   scale_x_discrete(limits = rev, labels = scales::wrap_format(15)) +
#   theme(axis.text.x = element_text(angle = 45))

ggplot(head(journal_df, 20), aes(n, prism_publication_name)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(20)) +
  labs(y = NULL) +
  theme_minimal()

```

Environmental Justice journal established in March 2008


## Publications through time

I will annotate this with key events like the Kyoto protocol, IPCC publications...

```{r echo=FALSE, message=FALSE}
publications_by_year <- author_df |> 
  select(year) |>
  group_by(year) |> 
  summarise(count = length(year))

ggplot(publications_by_year, aes(x = year, y = count)) +
  geom_line() +
  scale_x_date(limits = c(publications_by_year$year[1], "2020-01-01")) +
  theme_minimal()

```


## Mean number of authors through time

```{r echo=FALSE}
mean_author_by_year <- author_df |> 
  select(year, author_count_total) |>
  group_by(year) |> 
  summarise(mean_auth = mean(author_count_total))

ggplot(mean_author_by_year, aes(x = year, y = mean_auth)) +
  geom_line() +
  theme_minimal()

```

## Most productive authors (first and not)

Issues with duplicate authors? I.e., with/without initials in first name?

```{r echo=FALSE, message=FALSE}
dim(scopus_results$author) # 4137 rows
sum(is.na(scopus_results$author$given_name)) # 28 NA
sum(is.na(scopus_results$author$surname)) # 21 NA
scopus_results$author[which(is.na(scopus_results$author$given_name)),]


# Am I having issues with duplicate authors? I.e., with/without initials in first name?
productive_authors_df <- scopus_results$author |> 
  drop_na(surname) |> # Most productive author is NA NA lol
  unite("full_name", c("surname", "given_name"), sep = ", ", remove = FALSE) |> 
  count(full_name, sort = TRUE) |>
  mutate(full_name = fct_reorder(full_name, n))

# unite can also be: mutate(full_name = str_c(scopus_results$author$given_name, scopus_results$author$surname, sep = ", "))

ggplot(head(productive_authors_df, 20), aes(n, full_name)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(15), na.translate = FALSE) +
  labs(y = NULL) +
  theme_minimal()

```

## Jacqui's suggestion: cleaning author names to handle duplicates by using "Surname, FirstInitial."

The most common and basic unit we can fall back to is "Surname, FirstInitial."

NOTE: This assumes that all combinations of names will produce a unique value, this may not always be the case (e.g. Jacqui Vanderhoorn, Jonathan Vanderhoorn)

```{r echo=FALSE, message=FALSE}

#scopus_results$author
#question: what is this dataframe? Is it all of the authors pulled from the papers, without cleaning?

top_authors <-
scopus_results$author %>%
  select(surname, initials) %>%
  #removing entries with no surname or first initial as they will add NAs into our string cleaning.
  #there are some entries with a surname and no other information, these will be removed. But if we are only interested in the top authors, they are not a problem
  drop_na() %>%
  #extracting first initial and pasting with surname in the format of "Surname, FirstInitial."
  mutate(name_formatted = paste(surname, substr(initials, start = 1, stop = 2), sep = ", ")) %>%
  #counting the number of publications 
  count(name_formatted, sort = TRUE) %>%
  #only including authors that have published more than 5 papers
  filter(n >= 5) %>%
  #converting author name to factor
  mutate(name_formatted = as.factor(name_formatted)) %>%
  #using fct_reorder2() to sort by two vlaues, 1: sorting by n, then 2: by alphabetical order
  mutate(name_formatted = fct_reorder2(name_formatted, n, sort(name_formatted)))

ggplot(data = top_authors) +
  geom_col(aes(x = n, y = name_formatted)) +
  labs(y = NULL) +
  theme_minimal()

```




## Most cited papers

```{r echo=FALSE, message=FALSE}
sum(is.na(author_df$citedby_count))

author_df |>
  arrange(desc(citedby_count)) |> 
  slice(1:10) |>
  mutate(dc_title = fct_reorder(dc_title, citedby_count)) |> 
  ggplot(aes(y = dc_title, x = citedby_count)) +
  geom_col() +
  scale_y_discrete(labels = scales::wrap_format(20)) +
  labs(y = NULL) +
  theme_minimal()

# Alternate way of doing the same thing:
# title_df <- author_df |>
#   select(dc_title, citedby_count) |> 
#   arrange(desc(citedby_count)) |> 
#   slice(1:15) |>
#   mutate(dc_title = fct_reorder(dc_title, citedby_count))
# 
# ggplot(title_df, aes(y = dc_title, x = citedby_count)) +
#   geom_col() +
#   scale_y_discrete(limits = rev, labels = scales::wrap_format(15)) +
#   labs(y = NULL) +
#   theme_minimal()

```



# Corpus analysis

## Most common words in abstracts.
Not surprisingly they match the search terms

```{r, echo=FALSE, message=FALSE}
# Tutorial groups by book does not work well for bigrams
# https://bookdown.org/Maxine/tidy-text-mining/the-unnest-tokens-function.html
# Does tokenize do lemmitization?

tidy_abs <- author_df |>
  select(entry_number, dc_description) |> 
  # group_by(entry_number) |>  # All abstracts not comparing publications
  unnest_tokens(word, dc_description) |> 
  anti_join(stop_words) |> 
  ungroup()

tidy_abs |>
  count(word, sort = T) |>
  filter(n > 600) |>
  mutate(word = reorder(word, n)) |>
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

# Top n words
# tidy_abs |> 
#   count(word, sort = TRUE) |>
#   top_n(10) |> 
#   mutate(word = reorder(word, n)) |>
#   ggplot() +
#   geom_col(aes(x = n, y = word))

```

## Most common word pairs in abstracts (again, no surprises)

```{r, echo=FALSE, message=FALSE}

# bigrams does not work well grouped by entry (too little information in abs)
# tutorial does not group_by book https://bookdown.org/Maxine/tidy-text-mining/tokenizing-by-n-gram.html
tidy_abs_ngrams <- author_df |>
  select(entry_number, dc_description) |> 
  unnest_tokens(bigram, dc_description, token = "ngrams", n = 2) |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% c(stop_words$word, NA),
         !word2 %in% c(stop_words$word, NA)) |>
  unite(bigram, c(word1, word2), sep = " ") |>
  count(bigram, sort = T) 

tidy_abs_ngrams |>
  slice(1:20) |>
  mutate(bigram = reorder(bigram, n)) |>
  ggplot(aes(n, bigram)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

```



## Network of word pairs

Network graph of most commonly joined words. "Note that this is a visualization of a Markov chain, a common model in text processing, where the choice of a word only depends on its previous word".

```{r echo=FALSE}
tidy_abs_ngrams_net <- author_df |>
  select(entry_number, dc_description) |> 
  unnest_tokens(bigram, dc_description, token = "ngrams", n = 2) |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% c(stop_words$word, NA),
         !word2 %in% c(stop_words$word, NA)) |>
  unite(bigram, c(word1, word2), sep = " ", remove = FALSE) |>
  count(word1, word2, sort = T) 

bigram_graph <- tidy_abs_ngrams_net %>% 
  filter(n > 20) %>%
  as_tbl_graph()

arrow <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") + 
  geom_edge_link(aes(alpha = n), show.legend = F, 
                 arrow = arrow, end_cap = circle(0.07, "inches")) + 
  geom_node_point(color = "lightblue", size = 5) + 
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

# tf-idf and log ratio

Compares documents, not useful in this case but curious. Maybe can be analysed further?
Did not work when applied to all abstracts not grouped by article.

```{r, echo=FALSE, message=FALSE}
# The following compares documents. Not useful in this case.
tf_idf <- author_df |>
  select(entry_number, dc_description) |>
  unnest_tokens(word, dc_description) |>
  add_count(entry_number, name = "total_words") |>
  group_by(entry_number, total_words) |>
  count(word, sort = TRUE) |>
  ungroup() |>
  select(-total_words) |>
  # mutate(all = "all_abs") |>
  bind_tf_idf(term = word, document = entry_number, n = n) |>
  bind_log_odds(set = entry_number, feature = word, n = n) |>
  arrange(desc(tf_idf))

head(tf_idf)

```

A word cloud (because why not?)

```{r, echo=FALSE, message=FALSE}

tidy_abs |>
  count(word, sort = T) |> 
  top_n(200) |> 
  ggplot() +
  geom_text_wordcloud_area(aes(label = word, size = n)) +
  scale_size_area(max_size = 15) +
  theme_light()


```


# Correlating word pairs

```{r, echo=FALSE, message=TRUE}
tidy_abs_widyr <- tidy_abs |>
  pairwise_count(word, entry_number, sort = TRUE)

tidy_abs_widyr_word_cors <- tidy_abs |> 
  add_count(word) |> 
  filter(n >= 20) |> 
  select(-n) |>
  pairwise_cor(word, entry_number, sort = TRUE) # Grouping by entry number. same problem as bigrams, too little info in abstracts?

# Using search terms, issues with contractions?
tidy_abs_widyr_word_cors |>
  filter(item1 %in% c("climate", "change", "justice", "social",
                      "environmental", "global", "warming")) |>
  group_by(item1) |>
  top_n(6) |>
  ungroup() |>
  facet_bar(y = item2, x = correlation, by = item1, nrow = 7)

```

Do I need to do some regex to remove numbers from abstracts? looks like DOIs are in there. Same from grouped bigrams.

```{r, echo=FALSE, message=TRUE}
# problem with grouping variable same as bigrams. see next code chunk
tidy_abs_widyr_word_cors %>%
  filter(correlation > .5) %>%
  as_tbl_graph() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE)

```

The following is bigrams grouped by article rather than all article abstracts. Probably not useful to analyse per abstract but highlights some possible filtering is necessary on abstract texts?

```{r, echo=FALSE, message=FALSE}
# Numbers, DOIs dates?
x <- author_df |>
  select(entry_number, dc_description) |>
  group_by(entry_number) |>
  unnest_tokens(bigram, dc_description, token = "ngrams", n = 2) |> 
  separate(bigram, into = c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% c(stop_words$word, NA),
         !word2 %in% c(stop_words$word, NA)) |>
  unite(bigram, c(word1, word2), sep = " ") |>
  count(bigram, sort = T) |>
  ungroup() |>
  slice(1:20) |>
  mutate(bigram = reorder(bigram, n))

ggplot(x, aes(n, bigram)) +
  geom_col() +
  labs(y = NULL) +
  theme_minimal()

```

# TO DO

## Author collaboration netrowk

Not sure how to do this yet. George has some code in dropbox I need to have a look at

## Futher analysis (ordination...?)

## Countries publishing most

## Countries most often mentioned in abstracts/keywords?


## Analysis within topic (LDA possible within the given topic?)


## Look at Finn and Georges Janky code





